#Thinking1	机器学习中的监督学习、非监督学习、强化学习有何区别#

答：监督学习数据中会有label，在训练之前就有反馈，但是强化学习是没有label的，但是会有奖励值，而且奖励值也是之后在训练过程中才会有，而不是事先就有。
非监督学习是没有label的，也没有奖励值，但是强化学习是有奖励值的。其次非监督学习中的数据之间时相对独立的数据，但是强化学习中的数据具有前后关系性。

# Thinking2	什么是策略网络，价值网络，有何区别 #

答：
基于价值的强化学习，Value-Based
就是根据结果，对其进行打分，找到最优的分数，然后进行反馈，基于分数，进行奖励。

基于策略强化学习，Policy-Based
是基于当前状态，对下一时刻可进行的选择计算每个选择的概率，进行策略的选择。
最后可得出基于什么状态，可利用什么策略

# Thinking3	请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的 #

答：
（1）选择 ：从根节点往下走，每次都选择一个分数较高的节点，然后直到存在为扩展的节点（也就是说每次选择都会产生一个新的为扩展的子节点）
（2）拓展：然后将上步新扩展的节点进行创建，如果是刚开始，没有根节点，则拓展一个节点
（3）模拟：对节点采用随机的方式，快速模拟若干次实验，每次实验都模拟到终结状态（得到结果），可得到当前模拟的分数。
（4）回传：根据节点若干次模拟的分数，更新当前节点以及所有祖先节点的分数值。


# Thinking4：	假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑 #

答：其实可以将用户看作agent，抖音的推荐视频看作environment,用户的各种操作，比如上滑，关注，点赞等看作action，当用户观看时长或是点击率超过一定限度，可看作奖励，直接上滑或者取消关注等看作负反馈，等看作强化学习的要素。

# Thinking5：	在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路 #

答：其实这里将车辆看作agent，将车辆的动作（加速，减速，转弯等）看作aciton，将周围环境看作environment，每次执行动作，用时更短到达目的地看作奖励，未到达目的地或时间过长到达目的地看作负反馈。
在学习中，每一步的行动都将获得环境给予的反馈，通过反馈，不断调整训练对象的行为。

# Action	"Action（五子棋）：请说明都有哪些模块，不同模块的原理 #
答：首先得有一个神经网络部分，将历史记录和棋盘states作为输入，输出下一步的落子概率，这个神经网络部分包括策略网络和价值网络。其次，光有概率是不够的，因为这样的话。神经网络只会不断的选择最大的那个概率，然后那个概率也会因为被选择而越来越大，这样的话就死循环了，也没有价值了，所以还要有一个MCTS来进行一个策略优化，使得每个策略都会被考虑到的情况下再择优。接下来作为下一次的学习资料作为循环再次学习。
